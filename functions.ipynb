{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import numpy as np\n",
    "import fasttext\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(dataframe, labels, method, perc_training, num_trainingsamples):\n",
    "#     Creates a radomly-sampled train/test split for the provided data (dataframe and labels). Two methods are possible:\n",
    "#     method = 0: by specified percentage. The size of the training data will be len(data)*perc_training\n",
    "#     method = 1: by number of cases. The size of the training data will be num_trainingsamples\n",
    "    \n",
    "    if(method==0):\n",
    "        num_trainingsamples = int(round(len(dataframe)*perc_training))\n",
    "        \n",
    "    msk = [False]*(len(dataframe)-num_trainingsamples) + [True]*num_trainingsamples\n",
    "    shuffle(msk)\n",
    "    msk = np.asarray(msk)\n",
    "    \n",
    "    data_train = dataframe[msk]\n",
    "    data_test = dataframe[~msk]\n",
    "    labels_train = labels[msk]\n",
    "    labels_test = labels[~msk]\n",
    "    return data_train, data_test, labels_train, labels_test, msk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich(df):\n",
    "#     Takes the dataframe as loaded from the .csv and enriches it with some features\n",
    "       \n",
    "    data = df.copy()\n",
    "    \n",
    "    #Adding some extra features\n",
    "    data['distinct_ratio'] = (data['NUM_DISTINCT'] / (data['NUM_ROWS'] - data['NUM_NULLS'])).replace(np.inf, 0)\n",
    "    data['perc_null'] = (data['NUM_NULLS'] / data['NUM_ROWS']).replace(np.inf, 0)\n",
    "    data['is_id'] = np.where(data['COLUMN_NAME'].str.endswith('ID'), 1, 0)\n",
    "    \n",
    "    #Transforming the data type into 4 booleans (might have to be changed for data sources other than ORACLE)\n",
    "    data['is_number'] = np.where(data['DATA_TYPE']=='NUMBER', 1, 0)\n",
    "    data['is_character'] = np.where(data['DATA_TYPE'].isin(['VARCHAR2','BLOB','CLOB']), 1, 0)\n",
    "    data['is_date'] = np.where(data['DATA_TYPE'].isin(['DATE','TIMESTAMP(6)']), 1, 0)\n",
    "    data['is_large_blob'] = np.where(data['DATA_LENGTH']>=1000, 1, 0)\n",
    "    \n",
    "    #Drop columns that will not be used in the model\n",
    "    data = data.drop(['DATA_TYPE'], axis=1)\n",
    "    data = data.drop(['NUM_DISTINCT'], axis=1)    \n",
    "    data = data.drop(['NUM_NULLS'], axis=1)\n",
    "    data = data.drop(['DATA_LENGTH'], axis=1)\n",
    "    \n",
    "    #Fill the cells that contain NaN\n",
    "    data=data.fillna(0)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions related to training and using the text classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_to_embed(labels,pre_embedding_file):\n",
    "#     Takes the column names and labels and puts them in the format needed by FastText. They are temporarily saved in a .csv\n",
    "    df = labels[['PERSONAL_DATA','COLUMN_NAME']].copy()\n",
    "    df['COLUMN_NAME'] = df['COLUMN_NAME'].str.replace('_', '', regex=True)\n",
    "    df.insert(loc=0, column='aux', value = '__label__')\n",
    "    df.insert(loc=2, column='aux2', value = ' ')\n",
    "    df['final'] = df['aux'] + df['PERSONAL_DATA'].astype(str) + df['aux2'] + df['COLUMN_NAME']\n",
    "    df['final'].to_csv(pre_embedding_file, index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embedding_model(labels,column_to_embed):\n",
    "#     Trains the text classifier based on the labels and text column provided\n",
    "    prepare_to_embed(labels,'pre_embedding.csv')\n",
    "    model = fasttext.train_supervised(input=\"pre_embedding.csv\", epoch = 10, minn=1, maxn=10, lr = 1)\n",
    "    os.remove('pre_embedding.csv')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(word,model):\n",
    "#     Outputs the class prediction of the text classifier\n",
    "    if(model.predict(word)[0][0] == '__label__0'):\n",
    "        return 0\n",
    "    if(model.predict(word)[0][0] == '__label__1'):\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_embedding_to_df(data,model,column_to_embed):\n",
    "#     Takes a string column from a dataframe, uses the given text classifier model to create a class prediction, and\n",
    "#     adds a column to the dataframe with said prediction\n",
    "    data['embedding_pred'] = np.vectorize(predict_label)(data[column_to_embed],model)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Functions related to training the main prediction model (by default, a Random Forest Classifier):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(training_data, labels, model_type, estimators):\n",
    "#     Trains a machine learning model with the given data and labels\n",
    "#     The best results (featured in the paper) were obtained with a random forest (model_type = 1, default)\n",
    "    if model_type not in range(1,8):\n",
    "        model_type = 1 #by default, random forest\n",
    "    \n",
    "    if model_type == 1: #Random Forest\n",
    "        ml_model = RandomForestClassifier(n_estimators = estimators)\n",
    "    elif model_type == 2: #Logistic Regression\n",
    "        ml_model = LogisticRegression(solver='liblinear')\n",
    "    elif model_type == 3: #Gaussian Naive Bayes\n",
    "        ml_model = GaussianNB()\n",
    "    elif model_type == 4: #Ada Boost\n",
    "        ml_model = AdaBoostClassifier(n_estimators=estimators)\n",
    "    elif model_type == 5: #Gradient Boosting\n",
    "        ml_model = GradientBoostingClassifier(n_estimators=estimators, learning_rate=1.0, max_depth=3, random_state=0)\n",
    "    elif model_type == 6: #KNN classifier\n",
    "        ml_model = KNeighborsClassifier(n_neighbors=9)\n",
    "        \n",
    "    ml_model.fit(training_data, labels.values.ravel())\n",
    "    return ml_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_importances_df(model,data):\n",
    "    #Returns a dataframe with the feature importances\n",
    "    return pd.DataFrame(model.feature_importances_,index = data.columns, columns=['importance']).sort_values('importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_test):\n",
    "    #Returns a dataframe with the predictions made by the model on the data_test\n",
    "    return pd.DataFrame(model.predict_proba(data_test)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prediction(test_columns,predictions,labels,threshold,results_file):\n",
    "#     Compares the predictions iwth the labels\n",
    "#     Returns the precision and recall and f1 based on the provided threshold, as well as the ROC AUC score\n",
    "#     Saves the results to the results_file, where they can be analyzed\n",
    "    to_eval = labels.reset_index().join(predictions.reset_index().drop(['index'], axis=1)).drop(['index'], axis=1)\n",
    "    to_eval.columns = ['labels','Prediction']\n",
    "    to_eval['Prediction (binary)'] = np.where(to_eval['Prediction']>=threshold, 1, 0)\n",
    "    to_write = test_columns.reset_index().join(to_eval.reset_index().drop(['index'], axis=1))\n",
    "    to_write.to_csv(results_file, index=False)\n",
    "    \n",
    "    if len(to_eval[to_eval['Prediction (binary)'] == 1]) > 0:\n",
    "        precision = len(to_eval[(to_eval['labels'] == 1) & (to_eval['Prediction (binary)'] == 1)])/len(to_eval[to_eval['Prediction (binary)'] == 1])\n",
    "    else:\n",
    "        precision = 0\n",
    "    \n",
    "    if len(to_eval[to_eval['labels'] == 1]) > 0: \n",
    "        recall = len(to_eval[(to_eval['labels'] == 1) & (to_eval['Prediction (binary)'] == 1)])/len(to_eval[to_eval['labels'] == 1])\n",
    "    else:\n",
    "        recall = 0\n",
    "    \n",
    "    if ((precision >0) and (recall> 0)):\n",
    "        f1 = 2*(precision*recall)/(precision+recall)\n",
    "    else:\n",
    "        f1 = 0\n",
    "        \n",
    "    auc = roc_auc_score(to_eval['labels'], to_eval['Prediction (binary)'])\n",
    "    \n",
    "    return precision,recall,f1,auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions related to the \"Advanced Options\" section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_run(specify_test_data, split_method, perc_training, num_trainingsamples, columns_to_use, model_type, results_file, num_tests):\n",
    "    data = pd.read_csv(metadata_file)\n",
    "    labels = pd.read_csv(labels_file)\n",
    "\n",
    "    #OPTIONAL, but used for the results in the paper:\n",
    "    data = data[data['NUM_DISTINCT'] > 0] #We'll only analyze columns for which there were some entries\n",
    "    labels = labels[labels.index.isin(data.index)] #Make sure the data and labels match\n",
    "\n",
    "    precision_list = list()\n",
    "    recall_list = list()\n",
    "    f1_list = list()\n",
    "    auc_list = list()\n",
    "    \n",
    "    for i in range(0,num_tests):\n",
    "        if(specify_test_data == 0):\n",
    "            data_train, data_test, labels_train, labels_test, msk = train_test_split(data, labels, split_method, perc_training, num_trainingsamples)\n",
    "        elif(specify_test_data == 1):\n",
    "            data_train = data\n",
    "            labels_train = labels\n",
    "            data_test = pd.read_csv(metadata_file_test)\n",
    "            labels_test = pd.read_csv(labels_file_test)\n",
    "            data_test = data_test[data_test['NUM_DISTINCT'] > 0]\n",
    "            labels_test = labels_test[labels_test.index.isin(data_test.index)]\n",
    "\n",
    "        data_train = enrich(data_train)\n",
    "        data_test = enrich(data_test)\n",
    "\n",
    "        if('embedding_pred' in columns_to_use):\n",
    "            embed_model = train_embedding_model(labels_train,'COLUMN_NAME')\n",
    "            data_train = add_embedding_to_df(data_train,embed_model,'COLUMN_NAME')\n",
    "            data_test = add_embedding_to_df(data_test,embed_model,'COLUMN_NAME')\n",
    "\n",
    "        test_columns = labels_test[['TABLE_NAME','COLUMN_NAME']]\n",
    "        data_train = data_train[columns_to_use]\n",
    "        data_test = data_test[columns_to_use]\n",
    "        labels_train = labels_train.drop(['TABLE_NAME','COLUMN_NAME'], axis=1)\n",
    "        labels_test = labels_test.drop(['TABLE_NAME','COLUMN_NAME'], axis=1)\n",
    "\n",
    "        ml_model = train_model(data_train, labels_train, model_type, 100)\n",
    "        predictions = predict(ml_model, data_test)\n",
    "\n",
    "        precision,recall,f1,auc = evaluate_prediction(test_columns,predictions,labels_test,0.5,results_file)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        f1_list.append(f1)\n",
    "        auc_list.append(auc)\n",
    "        \n",
    "    mean_precision = sum(precision_list)/len(precision_list)\n",
    "    mean_recall = sum(recall_list)/len(recall_list)\n",
    "    mean_f1 = sum(f1_list)/len(f1_list)\n",
    "    mean_auc = sum(auc_list)/len(auc_list)\n",
    "    \n",
    "    return mean_precision, mean_recall, mean_f1, mean_auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
